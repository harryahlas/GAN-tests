{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnistgan_test.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NQdpp84SMur4","colab_type":"text"},"source":["https://github.com/eriklindernoren/Keras-GAN/blob/master/dcgan/dcgan.py"]},{"cell_type":"code","metadata":{"id":"UFLf5wG-uSyh","colab_type":"code","colab":{}},"source":["import os\n","os.mkdir(\"images\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sC4J_6i9uSse","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":66},"outputId":"9a22db3b-1970-4b3d-9c2b-e81bbdf2c68e","executionInfo":{"status":"ok","timestamp":1568347426666,"user_tz":420,"elapsed":5870,"user":{"displayName":"harry ahlas","photoUrl":"","userId":"11515999622722943475"}}},"source":["!pip install Pillow==5.0.0\n","!pip install scipy==1.0.1"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: Pillow==5.0.0 in /usr/local/lib/python3.6/dist-packages (5.0.0)\n","Requirement already satisfied: scipy==1.0.1 in /usr/local/lib/python3.6/dist-packages (1.0.1)\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.0.1) (1.16.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9Wf1DyyJjDhR","colab_type":"code","colab":{}},"source":["# mnist to images pre\n","import pandas as pd\n","import numpy as np\n","df = pd.read_csv(\"sample_data/mnist_test.csv\")\n","images = df.iloc[:,1:].to_numpy()\n","labels = df.iloc[:,0].values.tolist()\n","print(images.shape)\n","#images.head(2)\n","images[1,:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DfRgk8p2tLuC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":83},"outputId":"5b2a2b72-b545-4db5-ffd6-f89547d3ebcf","executionInfo":{"status":"ok","timestamp":1568348070711,"user_tz":420,"elapsed":3356,"user":{"displayName":"harry ahlas","photoUrl":"","userId":"11515999622722943475"}}},"source":["import scipy.misc\n","import numpy as np\n","\n","\n","# def output_png(images, labels, prefix):\n","#     for i in range(len(images)):\n","#         out = os.path.join(prefix, \"%06d-num%d.png\"%(i,labels[i]))\n","#         scipy.misc.imsave(out, np.array(images[i]).reshape(28,28))\n","\n","for i in range(len(images)):\n","  out = str(\"images/mnist_\" + str(i) + \".png\")\n","  scipy.misc.imsave(out, np.array(images[i]).reshape(28,28))        "],"execution_count":21,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `imsave` is deprecated!\n","`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","Use ``imageio.imwrite`` instead.\n","  if sys.path[0] == '':\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"4mf1R6XJnRDh","colab_type":"code","colab":{}},"source":["\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gtcGvt6hjDpA","colab_type":"code","colab":{}},"source":["batch_size=128\n","sample_interval=50\n","#(X_train, xx), (xxx,xxxx)  = mnist.load_data()\n","(X_train, _), (_, _) = mnist.load_data()\n","         # Rescale -1 to 1\n","X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n","X_train = np.expand_dims(X_train, axis=3)\n","\n","# Adversarial ground truths\n","valid = np.ones((batch_size, 1))\n","fake = np.zeros((batch_size, 1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfjkbfVpslWz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FJLPqq19jp_x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"86775718-6f63-4fb5-bd47-ab349aabfd77","executionInfo":{"status":"ok","timestamp":1568343576935,"user_tz":420,"elapsed":567,"user":{"displayName":"harry ahlas","photoUrl":"","userId":"11515999622722943475"}}},"source":["print(X_train.shape,\n","df.shape,\n","28*28\n",")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["(60000, 28, 28) (9999, 785) 784\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JbmNSkCWjTli","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":246},"outputId":"d29c7d15-c575-4212-8e9d-029a3ffa66bb","executionInfo":{"status":"ok","timestamp":1568343413476,"user_tz":420,"elapsed":1270,"user":{"displayName":"harry ahlas","photoUrl":"","userId":"11515999622722943475"}}},"source":["df.head()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>7</th>\n","      <th>0</th>\n","      <th>0.1</th>\n","      <th>0.2</th>\n","      <th>0.3</th>\n","      <th>0.4</th>\n","      <th>0.5</th>\n","      <th>0.6</th>\n","      <th>0.7</th>\n","      <th>0.8</th>\n","      <th>0.9</th>\n","      <th>0.10</th>\n","      <th>0.11</th>\n","      <th>0.12</th>\n","      <th>0.13</th>\n","      <th>0.14</th>\n","      <th>0.15</th>\n","      <th>0.16</th>\n","      <th>0.17</th>\n","      <th>0.18</th>\n","      <th>0.19</th>\n","      <th>0.20</th>\n","      <th>0.21</th>\n","      <th>0.22</th>\n","      <th>0.23</th>\n","      <th>0.24</th>\n","      <th>0.25</th>\n","      <th>0.26</th>\n","      <th>0.27</th>\n","      <th>0.28</th>\n","      <th>0.29</th>\n","      <th>0.30</th>\n","      <th>0.31</th>\n","      <th>0.32</th>\n","      <th>0.33</th>\n","      <th>0.34</th>\n","      <th>0.35</th>\n","      <th>0.36</th>\n","      <th>0.37</th>\n","      <th>0.38</th>\n","      <th>...</th>\n","      <th>0.628</th>\n","      <th>0.629</th>\n","      <th>0.630</th>\n","      <th>0.631</th>\n","      <th>0.632</th>\n","      <th>0.633</th>\n","      <th>0.634</th>\n","      <th>0.635</th>\n","      <th>0.636</th>\n","      <th>0.637</th>\n","      <th>0.638</th>\n","      <th>0.639</th>\n","      <th>0.640</th>\n","      <th>0.641</th>\n","      <th>0.642</th>\n","      <th>0.643</th>\n","      <th>0.644</th>\n","      <th>0.645</th>\n","      <th>0.646</th>\n","      <th>0.647</th>\n","      <th>0.648</th>\n","      <th>0.649</th>\n","      <th>0.650</th>\n","      <th>0.651</th>\n","      <th>0.652</th>\n","      <th>0.653</th>\n","      <th>0.654</th>\n","      <th>0.655</th>\n","      <th>0.656</th>\n","      <th>0.657</th>\n","      <th>0.658</th>\n","      <th>0.659</th>\n","      <th>0.660</th>\n","      <th>0.661</th>\n","      <th>0.662</th>\n","      <th>0.663</th>\n","      <th>0.664</th>\n","      <th>0.665</th>\n","      <th>0.666</th>\n","      <th>0.667</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 785 columns</p>\n","</div>"],"text/plain":["   7  0  0.1  0.2  0.3  0.4  ...  0.662  0.663  0.664  0.665  0.666  0.667\n","0  2  0    0    0    0    0  ...      0      0      0      0      0      0\n","1  1  0    0    0    0    0  ...      0      0      0      0      0      0\n","2  0  0    0    0    0    0  ...      0      0      0      0      0      0\n","3  4  0    0    0    0    0  ...      0      0      0      0      0      0\n","4  1  0    0    0    0    0  ...      0      0      0      0      0      0\n","\n","[5 rows x 785 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"wulpouuXOT6n","colab_type":"code","colab":{}},"source":["from keras.datasets import mnist"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aLexCeGRMkYH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"865592be-bac7-4190-87ff-bb95f61c96d7"},"source":["from __future__ import print_function, division\n","\n","from keras.datasets import mnist\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n","from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras.models import Sequential, Model\n","from keras.optimizers import Adam\n","\n","import matplotlib.pyplot as plt\n","\n","import sys\n","\n","import numpy as np\n","\n","class DCGAN():\n","    def __init__(self):\n","        # Input shape\n","        self.img_rows = 28\n","        self.img_cols = 28\n","        self.channels = 1\n","        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","        self.latent_dim = 100\n","\n","        optimizer = Adam(0.0002, 0.5)\n","\n","        # Build and compile the discriminator\n","        self.discriminator = self.build_discriminator()\n","        self.discriminator.compile(loss='binary_crossentropy',\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","\n","        # Build the generator\n","        self.generator = self.build_generator()\n","\n","        # The generator takes noise as input and generates imgs\n","        z = Input(shape=(self.latent_dim,))\n","        img = self.generator(z)\n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","\n","        # The discriminator takes generated images as input and determines validity\n","        valid = self.discriminator(img)\n","\n","        # The combined model  (stacked generator and discriminator)\n","        # Trains the generator to fool the discriminator\n","        self.combined = Model(z, valid)\n","        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","    def build_generator(self):\n","\n","        model = Sequential()\n","\n","        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n","        model.add(Reshape((7, 7, 128)))\n","        model.add(UpSampling2D())\n","        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Activation(\"relu\"))\n","        model.add(UpSampling2D())\n","        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Activation(\"relu\"))\n","        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n","        model.add(Activation(\"tanh\"))\n","\n","        model.summary()\n","\n","        noise = Input(shape=(self.latent_dim,))\n","        img = model(noise)\n","\n","        return Model(noise, img)\n","\n","    def build_discriminator(self):\n","\n","        model = Sequential()\n","\n","        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n","        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        model.add(Flatten())\n","        model.add(Dense(1, activation='sigmoid'))\n","\n","        model.summary()\n","\n","        img = Input(shape=self.img_shape)\n","        validity = model(img)\n","\n","        return Model(img, validity)\n","\n","    def train(self, epochs, batch_size=128, save_interval=50):\n","\n","        # Load the dataset\n","        (X_train, _), (_, _) = mnist.load_data()\n","\n","        # Rescale -1 to 1\n","        X_train = X_train / 127.5 - 1.\n","        X_train = np.expand_dims(X_train, axis=3)\n","\n","        # Adversarial ground truths\n","        valid = np.ones((batch_size, 1))\n","        fake = np.zeros((batch_size, 1))\n","\n","        for epoch in range(epochs):\n","\n","            # ---------------------\n","            #  Train Discriminator\n","            # ---------------------\n","\n","            # Select a random half of images\n","            idx = np.random.randint(0, X_train.shape[0], batch_size)\n","            imgs = X_train[idx]\n","\n","            # Sample noise and generate a batch of new images\n","            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","            gen_imgs = self.generator.predict(noise)\n","\n","            # Train the discriminator (real classified as ones and generated as zeros)\n","            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n","            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n","            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","            # ---------------------\n","            #  Train Generator\n","            # ---------------------\n","\n","            # Train the generator (wants discriminator to mistake images as real)\n","            g_loss = self.combined.train_on_batch(noise, valid)\n","\n","            # Plot the progress\n","            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n","\n","            # If at save interval => save generated image samples\n","            if epoch % save_interval == 0:\n","                self.save_imgs(epoch)\n","\n","    def save_imgs(self, epoch):\n","        r, c = 5, 5\n","        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n","        gen_imgs = self.generator.predict(noise)\n","\n","        # Rescale images 0 - 1\n","        gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        fig.savefig(\"images/mnist_%d.png\" % epoch)\n","        plt.close()\n","\n","\n","if __name__ == '__main__':\n","    dcgan = DCGAN()\n","    dcgan.train(epochs=4000, batch_size=32, save_interval=50)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1 (Conv2D)            (None, 14, 14, 32)        320       \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 32)        0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n","_________________________________________________________________\n","zero_padding2d_1 (ZeroPaddin (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 8, 8, 64)          256       \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 4, 4, 128)         73856     \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 4, 4, 128)         512       \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 128)         0         \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 4, 4, 256)         1024      \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 4096)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 4097      \n","=================================================================\n","Total params: 393,729\n","Trainable params: 392,833\n","Non-trainable params: 896\n","_________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n","\n","Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_2 (Dense)              (None, 6272)              633472    \n","_________________________________________________________________\n","reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 14, 14, 128)       147584    \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 14, 14, 128)       512       \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 14, 14, 128)       0         \n","_________________________________________________________________\n","up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 28, 28, 64)        73792     \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 28, 28, 64)        256       \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 28, 28, 64)        0         \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 28, 28, 1)         577       \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 28, 28, 1)         0         \n","=================================================================\n","Total params: 856,193\n","Trainable params: 855,809\n","Non-trainable params: 384\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n","/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":["0 [D loss: 1.094324, acc.: 39.06%] [G loss: 0.573084]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":["1 [D loss: 0.681042, acc.: 64.06%] [G loss: 0.886264]\n","2 [D loss: 0.438854, acc.: 73.44%] [G loss: 1.485341]\n","3 [D loss: 0.220076, acc.: 96.88%] [G loss: 1.886354]\n","4 [D loss: 0.163416, acc.: 96.88%] [G loss: 1.365398]\n","5 [D loss: 0.138711, acc.: 95.31%] [G loss: 0.819737]\n","6 [D loss: 0.118652, acc.: 98.44%] [G loss: 0.560862]\n","7 [D loss: 0.176134, acc.: 95.31%] [G loss: 0.942513]\n","8 [D loss: 0.237953, acc.: 93.75%] [G loss: 1.448672]\n","9 [D loss: 0.578997, acc.: 71.88%] [G loss: 2.254769]\n","10 [D loss: 1.077041, acc.: 45.31%] [G loss: 2.391140]\n","11 [D loss: 0.980652, acc.: 42.19%] [G loss: 2.218510]\n","12 [D loss: 0.890395, acc.: 54.69%] [G loss: 1.877282]\n","13 [D loss: 0.640066, acc.: 65.62%] [G loss: 1.806910]\n","14 [D loss: 0.591175, acc.: 67.19%] [G loss: 1.695250]\n","15 [D loss: 0.733826, acc.: 60.94%] [G loss: 2.040248]\n","16 [D loss: 0.868866, acc.: 57.81%] [G loss: 0.965834]\n","17 [D loss: 0.370901, acc.: 81.25%] [G loss: 0.891650]\n","18 [D loss: 0.340702, acc.: 85.94%] [G loss: 0.966970]\n","19 [D loss: 0.321252, acc.: 87.50%] [G loss: 0.636526]\n","20 [D loss: 0.704726, acc.: 62.50%] [G loss: 0.799101]\n","21 [D loss: 0.899068, acc.: 57.81%] [G loss: 1.519722]\n","22 [D loss: 1.246885, acc.: 39.06%] [G loss: 1.403256]\n","23 [D loss: 0.958201, acc.: 43.75%] [G loss: 1.381060]\n","24 [D loss: 0.748760, acc.: 65.62%] [G loss: 1.153316]\n","25 [D loss: 0.937310, acc.: 54.69%] [G loss: 1.184375]\n","26 [D loss: 0.756987, acc.: 62.50%] [G loss: 1.653213]\n","27 [D loss: 1.066983, acc.: 42.19%] [G loss: 1.473960]\n","28 [D loss: 0.747827, acc.: 62.50%] [G loss: 1.584956]\n","29 [D loss: 0.964899, acc.: 35.94%] [G loss: 0.941412]\n","30 [D loss: 0.778114, acc.: 57.81%] [G loss: 1.317145]\n","31 [D loss: 1.069298, acc.: 37.50%] [G loss: 1.372655]\n","32 [D loss: 1.082644, acc.: 43.75%] [G loss: 1.673793]\n","33 [D loss: 0.986279, acc.: 39.06%] [G loss: 1.403201]\n","34 [D loss: 0.984886, acc.: 45.31%] [G loss: 0.943020]\n","35 [D loss: 0.686434, acc.: 59.38%] [G loss: 1.088583]\n","36 [D loss: 0.759551, acc.: 59.38%] [G loss: 1.390066]\n","37 [D loss: 0.841477, acc.: 53.12%] [G loss: 1.205982]\n","38 [D loss: 0.953487, acc.: 39.06%] [G loss: 1.393950]\n","39 [D loss: 1.008023, acc.: 42.19%] [G loss: 1.550195]\n","40 [D loss: 0.986772, acc.: 46.88%] [G loss: 1.314641]\n","41 [D loss: 0.871030, acc.: 42.19%] [G loss: 1.507067]\n","42 [D loss: 0.759296, acc.: 56.25%] [G loss: 1.517468]\n","43 [D loss: 1.067676, acc.: 32.81%] [G loss: 1.642694]\n","44 [D loss: 0.910307, acc.: 48.44%] [G loss: 1.286656]\n","45 [D loss: 0.833800, acc.: 50.00%] [G loss: 1.196291]\n","46 [D loss: 0.698209, acc.: 60.94%] [G loss: 1.161726]\n","47 [D loss: 0.726580, acc.: 57.81%] [G loss: 0.766550]\n","48 [D loss: 0.605901, acc.: 68.75%] [G loss: 0.941096]\n","49 [D loss: 0.715440, acc.: 53.12%] [G loss: 1.132302]\n","50 [D loss: 0.996313, acc.: 39.06%] [G loss: 1.474819]\n","51 [D loss: 1.139390, acc.: 34.38%] [G loss: 1.195655]\n","52 [D loss: 1.052094, acc.: 39.06%] [G loss: 1.448503]\n","53 [D loss: 0.875108, acc.: 42.19%] [G loss: 1.619669]\n","54 [D loss: 0.911303, acc.: 43.75%] [G loss: 1.446820]\n","55 [D loss: 0.941570, acc.: 48.44%] [G loss: 1.459085]\n","56 [D loss: 0.792367, acc.: 59.38%] [G loss: 1.501901]\n","57 [D loss: 0.971687, acc.: 48.44%] [G loss: 1.199830]\n","58 [D loss: 0.960056, acc.: 40.62%] [G loss: 1.509095]\n","59 [D loss: 0.855832, acc.: 46.88%] [G loss: 1.353876]\n","60 [D loss: 0.818288, acc.: 53.12%] [G loss: 1.252614]\n","61 [D loss: 0.912336, acc.: 54.69%] [G loss: 1.114563]\n","62 [D loss: 0.919396, acc.: 46.88%] [G loss: 1.373087]\n","63 [D loss: 0.764600, acc.: 60.94%] [G loss: 1.525514]\n","64 [D loss: 0.696896, acc.: 65.62%] [G loss: 1.225340]\n","65 [D loss: 0.813719, acc.: 50.00%] [G loss: 1.379511]\n","66 [D loss: 0.873989, acc.: 45.31%] [G loss: 1.287670]\n","67 [D loss: 0.936674, acc.: 39.06%] [G loss: 1.216741]\n","68 [D loss: 0.730373, acc.: 59.38%] [G loss: 1.250512]\n","69 [D loss: 0.803471, acc.: 53.12%] [G loss: 1.474969]\n","70 [D loss: 0.850701, acc.: 53.12%] [G loss: 1.324423]\n","71 [D loss: 0.900200, acc.: 48.44%] [G loss: 1.231185]\n","72 [D loss: 0.771160, acc.: 51.56%] [G loss: 1.613606]\n","73 [D loss: 0.805751, acc.: 48.44%] [G loss: 1.432585]\n","74 [D loss: 0.842967, acc.: 48.44%] [G loss: 1.421738]\n","75 [D loss: 0.963706, acc.: 42.19%] [G loss: 1.265263]\n","76 [D loss: 0.972914, acc.: 43.75%] [G loss: 1.468430]\n","77 [D loss: 0.695625, acc.: 67.19%] [G loss: 1.262218]\n","78 [D loss: 0.793629, acc.: 50.00%] [G loss: 1.403255]\n","79 [D loss: 0.987091, acc.: 45.31%] [G loss: 1.130789]\n","80 [D loss: 0.973530, acc.: 39.06%] [G loss: 1.203346]\n","81 [D loss: 0.862092, acc.: 48.44%] [G loss: 1.372816]\n","82 [D loss: 0.937894, acc.: 45.31%] [G loss: 1.084576]\n","83 [D loss: 0.919051, acc.: 42.19%] [G loss: 1.111259]\n","84 [D loss: 1.030701, acc.: 32.81%] [G loss: 1.555844]\n","85 [D loss: 0.654405, acc.: 65.62%] [G loss: 1.439152]\n","86 [D loss: 0.783788, acc.: 54.69%] [G loss: 1.058531]\n","87 [D loss: 0.883258, acc.: 48.44%] [G loss: 1.239108]\n","88 [D loss: 1.123590, acc.: 39.06%] [G loss: 1.058548]\n","89 [D loss: 0.902886, acc.: 45.31%] [G loss: 1.518844]\n","90 [D loss: 0.896243, acc.: 43.75%] [G loss: 1.212979]\n","91 [D loss: 0.722126, acc.: 59.38%] [G loss: 1.276377]\n","92 [D loss: 0.944253, acc.: 43.75%] [G loss: 1.165708]\n","93 [D loss: 0.902876, acc.: 46.88%] [G loss: 1.312567]\n","94 [D loss: 0.929690, acc.: 34.38%] [G loss: 1.191248]\n","95 [D loss: 0.844517, acc.: 56.25%] [G loss: 1.182572]\n","96 [D loss: 0.894261, acc.: 50.00%] [G loss: 1.183536]\n","97 [D loss: 0.900802, acc.: 48.44%] [G loss: 1.050640]\n","98 [D loss: 0.784526, acc.: 51.56%] [G loss: 0.942652]\n","99 [D loss: 0.812745, acc.: 54.69%] [G loss: 1.159943]\n","100 [D loss: 0.999666, acc.: 34.38%] [G loss: 1.298372]\n","101 [D loss: 0.859544, acc.: 53.12%] [G loss: 1.434423]\n","102 [D loss: 0.786115, acc.: 51.56%] [G loss: 1.281412]\n","103 [D loss: 0.973567, acc.: 45.31%] [G loss: 1.042325]\n","104 [D loss: 0.792907, acc.: 56.25%] [G loss: 1.195961]\n","105 [D loss: 1.012343, acc.: 37.50%] [G loss: 1.124386]\n","106 [D loss: 0.832334, acc.: 46.88%] [G loss: 1.208938]\n","107 [D loss: 0.931046, acc.: 50.00%] [G loss: 0.988995]\n","108 [D loss: 0.889152, acc.: 32.81%] [G loss: 1.198857]\n","109 [D loss: 0.862197, acc.: 40.62%] [G loss: 1.029018]\n","110 [D loss: 0.994356, acc.: 42.19%] [G loss: 1.057570]\n","111 [D loss: 0.846547, acc.: 48.44%] [G loss: 0.936171]\n","112 [D loss: 0.895236, acc.: 50.00%] [G loss: 1.037882]\n","113 [D loss: 0.950294, acc.: 43.75%] [G loss: 1.173769]\n","114 [D loss: 0.844906, acc.: 50.00%] [G loss: 1.190088]\n","115 [D loss: 0.826196, acc.: 51.56%] [G loss: 1.352677]\n","116 [D loss: 0.827770, acc.: 51.56%] [G loss: 1.315310]\n","117 [D loss: 0.950449, acc.: 42.19%] [G loss: 1.177518]\n","118 [D loss: 0.762688, acc.: 53.12%] [G loss: 1.322495]\n","119 [D loss: 0.982474, acc.: 40.62%] [G loss: 1.109577]\n","120 [D loss: 0.818421, acc.: 54.69%] [G loss: 1.154341]\n","121 [D loss: 0.793774, acc.: 46.88%] [G loss: 1.220223]\n","122 [D loss: 0.849834, acc.: 46.88%] [G loss: 1.280202]\n","123 [D loss: 0.754266, acc.: 62.50%] [G loss: 1.288508]\n","124 [D loss: 0.896567, acc.: 32.81%] [G loss: 1.308517]\n","125 [D loss: 0.719259, acc.: 62.50%] [G loss: 1.102278]\n","126 [D loss: 0.826715, acc.: 45.31%] [G loss: 1.094287]\n","127 [D loss: 1.021058, acc.: 40.62%] [G loss: 1.213936]\n","128 [D loss: 0.830184, acc.: 50.00%] [G loss: 1.190266]\n","129 [D loss: 0.793114, acc.: 48.44%] [G loss: 1.329748]\n","130 [D loss: 0.995854, acc.: 37.50%] [G loss: 0.983980]\n","131 [D loss: 0.966361, acc.: 35.94%] [G loss: 1.208350]\n","132 [D loss: 0.750939, acc.: 67.19%] [G loss: 1.265601]\n","133 [D loss: 0.927102, acc.: 35.94%] [G loss: 1.079307]\n","134 [D loss: 0.757118, acc.: 57.81%] [G loss: 0.845269]\n","135 [D loss: 0.864761, acc.: 45.31%] [G loss: 1.056731]\n","136 [D loss: 0.850746, acc.: 50.00%] [G loss: 1.166975]\n","137 [D loss: 0.949545, acc.: 34.38%] [G loss: 1.011022]\n","138 [D loss: 0.887229, acc.: 48.44%] [G loss: 1.265353]\n","139 [D loss: 0.780493, acc.: 54.69%] [G loss: 1.061808]\n","140 [D loss: 0.833474, acc.: 43.75%] [G loss: 1.023893]\n","141 [D loss: 0.932160, acc.: 42.19%] [G loss: 1.332848]\n","142 [D loss: 0.888151, acc.: 42.19%] [G loss: 1.241469]\n","143 [D loss: 0.801784, acc.: 48.44%] [G loss: 1.175999]\n","144 [D loss: 0.770540, acc.: 51.56%] [G loss: 1.486149]\n","145 [D loss: 0.768042, acc.: 56.25%] [G loss: 1.034878]\n","146 [D loss: 0.889549, acc.: 35.94%] [G loss: 1.048544]\n","147 [D loss: 0.856725, acc.: 48.44%] [G loss: 1.207437]\n","148 [D loss: 0.816793, acc.: 46.88%] [G loss: 1.294220]\n","149 [D loss: 1.041065, acc.: 37.50%] [G loss: 1.075831]\n","150 [D loss: 0.830892, acc.: 46.88%] [G loss: 1.179237]\n","151 [D loss: 0.842070, acc.: 54.69%] [G loss: 1.116205]\n","152 [D loss: 0.816702, acc.: 45.31%] [G loss: 1.114063]\n"],"name":"stdout"}]}]}